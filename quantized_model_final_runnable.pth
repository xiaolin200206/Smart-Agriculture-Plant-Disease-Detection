import torch
import torch.nn as nn
import torchvision.models as models
from torch.nn.quantized import FloatFunctional
from torch.quantization import QuantStub, DeQuantStub, get_default_qat_qconfig, prepare, convert
import json
import os

print(">>> Starting to generate the final deployable model from best_model.pth...")

# --- 1. Ensure required files exist ---
class_names_path = "class_names.json"
float_model_path = "best_model.pth" # This is the “golden” file we need

if not os.path.exists(class_names_path):
    raise FileNotFoundError("Error: class_names.json file is required.")
if not os.path.exists(float_model_path):
    raise FileNotFoundError(f"Critical error: Original floating-point model '{float_model_path}' not found! This is the most important file.")

with open(class_names_path, 'r', encoding='utf-8') as f:
    num_classes = len(json.load(f))
print(f"[INFO] Found {num_classes} classes.")

# --- 2. Define a properly quantizable ResNet structure ---
class QuantizableBottleneck(models.resnet.Bottleneck):
    def __init__(self, *args, **kwargs):
        super(QuantizableBottleneck, self).__init__(*args, **kwargs)
        self.add_relu = FloatFunctional()
    def forward(self, x):
        identity = x
        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)
        out = self.conv2(out); out = self.bn2(out); out = self.relu(out)
        out = self.conv3(out); out = self.bn3(out)
        if self.downsample is not None: identity = self.downsample(x)
        out = self.add_relu.add_relu(out, identity)
        return out

class QuantizableResNet(models.resnet.ResNet):
    def __init__(self, *args, **kwargs):
        super(QuantizableResNet, self).__init__(block=QuantizableBottleneck, *args, **kwargs)

def create_quantizable_finetuned_resnet(num_classes: int):
    model = QuantizableResNet(layers=[3, 4, 6, 3])
    num_ftrs = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Linear(num_ftrs, 512), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(512, num_classes)
    )
    return model

# --- 3. Load the floating-point model and perform proper quantization ---
print(f"[INFO] Loading golden file: '{float_model_path}'...")
# Create a shell identical to the model used when training best_model.pth
# Note: Here we use standard models.resnet50 because best_model.pth was trained with it
standard_model_shell = models.resnet50(weights=None)
num_ftrs = standard_model_shell.fc.in_features
standard_model_shell.fc = nn.Sequential(
    nn.Linear(num_ftrs, 512), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(512, num_classes)
)
standard_model_shell.load_state_dict(torch.load(float_model_path, map_location="cpu"))
print("[SUCCESS] Original floating-point weights loaded successfully.")

print("[INFO] Transferring floating-point weights to the new quantizable model...")
final_quantizable_model = create_quantizable_finetuned_resnet(num_classes)
# This is safe because the layer names are compatible
final_quantizable_model.load_state_dict(standard_model_shell.state_dict())
print("[SUCCESS] Weights have been installed into the new quantizable backbone.")

print("[INFO] Performing final, correct quantization conversion...")
final_quantizable_model.eval()
model_to_quantize = nn.Sequential(QuantStub(), final_quantizable_model, DeQuantStub())
model_to_quantize.qconfig = get_default_qat_qconfig('fbgemm')
model_prepared = prepare(model_to_quantize, inplace=False)
model_converted = convert(model_prepared, inplace=False)
print("[SUCCESS] Model has been fully assembled and quantized.")

# --- 4. Save the final deployable quantized model ---
new_model_filename = "quantized_model_final_runnable.pth"
torch.save(model_converted.state_dict(), new_model_filename)

print("="*60)
print(f"✅✅✅ Final model successfully generated! ✅✅✅")
print(f"A brand-new, correct, deployable quantized model has been saved as: '{new_model_filename}'")
print("You can now use this file in your app.py for deployment.")
print("="*60)
